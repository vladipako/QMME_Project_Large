{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladipako/Transfer_Entropy_Project/blob/main/TE_Package.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from typing import Union\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from functools import partial\n",
        "import itertools\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import FancyArrowPatch, Circle\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "\n",
        "  def __init__(self, n_bins: int) -> None:\n",
        "    self.n_bins = n_bins\n",
        "\n",
        "  def discretize_returns(self, returns, method = 'quantile') -> pd.Series:\n",
        "    if method == 'uniform':\n",
        "        bins = np.linspace(np.min(returns), np.max(returns), self.n_bins + 1)\n",
        "        discretized = pd.cut(returns, bins=self.n_bins, labels=False, include_lowest=True)\n",
        "    elif method == 'quantile':\n",
        "        discretized = pd.qcut(returns, q=self.n_bins, labels=False, duplicates='drop')\n",
        "    elif method == 'pass':\n",
        "        discretized = returns\n",
        "    else:\n",
        "        raise ValueError(\"Discretization method must be either 'uniform' or 'quantile'\")\n",
        "\n",
        "    return pd.Series(discretized, index=getattr(returns, 'index', None))\n",
        "\n",
        "  def standard_scaler(self, series: pd.Series) -> pd.Series:\n",
        "    return (series - series.mean()) / series.std()\n"
      ],
      "metadata": {
        "id": "LCzho3JbKH3I"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataGenerator:\n",
        "\n",
        "  def __init__(self,\n",
        "               n_samples = 10_000,\n",
        "               x_scale = 1,\n",
        "               y_scale = 1,\n",
        "               ):\n",
        "\n",
        "    self.n_samples = n_samples\n",
        "    self.x_scale = x_scale\n",
        "    self.y_scale = y_scale\n",
        "\n",
        "  def randomData(self):\n",
        "    x = np.random.normal(loc = 0, scale = self.x_scale, size = self.n_samples)\n",
        "    y = np.random.normal(loc = 0, scale = self.y_scale, size = self.n_samples)\n",
        "    return pd.Series(x), pd.Series(y)\n",
        "\n",
        "  def simpleData(self):\n",
        "    x = np.random.normal(loc = 0, scale = self.x_scale, size = self.n_samples)\n",
        "    x_scaled = x * np.pi\n",
        "    y = np.sin(np.roll(x_scaled, 1)) + np.random.normal(loc = 0, scale = self.y_scale, size = self.n_samples)\n",
        "    y[0] = np.sin(x_scaled[-1]) + np.random.randn() * self.y_scale\n",
        "    return pd.Series(x), pd.Series(y)\n",
        "\n",
        "  def complexData(self):\n",
        "    x = np.random.normal(loc = 0, scale = self.x_scale, size = self.n_samples)\n",
        "    y = np.zeros(self.n_samples)\n",
        "\n",
        "    # Initialize the first two y values (as they depend on non-existent past x values)\n",
        "    y[0] = np.random.randn() * self.y_scale\n",
        "    y[1] = np.random.randn() * self.y_scale\n",
        "\n",
        "    for t in range(2, self.n_samples):\n",
        "      if x[t] > 0:\n",
        "          y[t] = np.sin(x[t-1]) + np.random.randn() * self.y_scale\n",
        "      else:\n",
        "          y[t] = np.cos(x[t-2])**2 + np.random.randn() * self.y_scale\n",
        "\n",
        "    return pd.Series(x), pd.Series(y)"
      ],
      "metadata": {
        "id": "QW_uxn9W_q2I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShuffleTest:\n",
        "  def __init__(self,\n",
        "               calculator,\n",
        "               mode: str,\n",
        "               x_series: Union[np.ndarray, pd.Series],\n",
        "               y_series: Union[np.ndarray, pd.Series],\n",
        "               lag: int,\n",
        "               n_shuffles: int = 300,\n",
        "               n_bins: int = 5,\n",
        "               discretization_method: str = 'quantile',\n",
        "               base: float = 2):\n",
        "\n",
        "      \"\"\"\n",
        "      Parameters:\n",
        "      x_series: Input series X\n",
        "      y_series: Input series Y\n",
        "      mode: Type of TE ('cumulative' or 'individual')\n",
        "      lag: Number of lags\n",
        "      n_shuffles: Number of shuffle iterations\n",
        "      n_bins: Number of bins for discretization\n",
        "      discretization_method: Method for discretization\n",
        "      base: Base for logarithm\n",
        "      \"\"\"\n",
        "\n",
        "      self.calculator = calculator\n",
        "      self.mode = mode\n",
        "      self.x_series = x_series\n",
        "      self.y_series = y_series\n",
        "      self.lag = lag\n",
        "      self.n_shuffles = n_shuffles\n",
        "      self.n_bins = n_bins\n",
        "      self.discretization_method = discretization_method\n",
        "      self.base = base\n",
        "\n",
        "      self.original_te = self.calculator.processor()\n",
        "\n",
        "  @staticmethod\n",
        "  def shuffle_series(series):\n",
        "      return np.random.permutation(series)\n",
        "\n",
        "  def perform_test(self):\n",
        "      \"\"\"Sequential shuffle test\"\"\"\n",
        "      shuffle_tes = []\n",
        "\n",
        "      for _ in tqdm(range(self.n_shuffles), desc='Running shuffle iterations'):\n",
        "          y_shuffled = self.shuffle_series(self.y_series)\n",
        "\n",
        "          model = TransferEntropy(\n",
        "              self.mode,\n",
        "              self.x_series,\n",
        "              y_shuffled,\n",
        "              self.lag,\n",
        "              n_bins=self.n_bins,\n",
        "              discretization_method=self.discretization_method,\n",
        "              base=self.base)\n",
        "\n",
        "          te = model.fit()['te_value']\n",
        "          shuffle_tes.append(te)\n",
        "\n",
        "      p_value = sum(te >= self.original_te for te in shuffle_tes) / self.n_shuffles\n",
        "      return shuffle_tes, p_value\n",
        "\n",
        "  def perform_parallel_test(self):\n",
        "    \"\"\"Parallel shuffle test\"\"\"\n",
        "    with Pool(processes=cpu_count()) as pool:\n",
        "        with tqdm(total=self.n_shuffles, desc='Running shuffle iterations', position=0, leave=True) as pbar:\n",
        "            shuffle_tes = []\n",
        "            for result in pool.imap_unordered(self._calculate_shuffled_te,\n",
        "                                            (self.shuffle_series(self.y_series) for _ in range(self.n_shuffles))):\n",
        "                shuffle_tes.append(result)\n",
        "                pbar.update()\n",
        "\n",
        "    p_value = sum(shuffle_te >= self.original_te for shuffle_te in shuffle_tes) / self.n_shuffles\n",
        "    return shuffle_tes, p_value\n",
        "\n",
        "  def _calculate_shuffled_te(self, y_shuffled):\n",
        "      \"\"\"Helper method for parallel processing\"\"\"\n",
        "\n",
        "      model = TransferEntropy(\n",
        "          self.mode,\n",
        "          self.x_series,\n",
        "          y_shuffled,\n",
        "          self.lag,\n",
        "          n_bins=self.n_bins,\n",
        "          discretization_method=self.discretization_method,\n",
        "          base=self.base)\n",
        "\n",
        "      return model.fit()['te_value']"
      ],
      "metadata": {
        "id": "vDjCginDLLs2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransferEntropy:\n",
        "\n",
        "  def __init__(self,\n",
        "               mode: str,\n",
        "               x_series: Union[np.ndarray, pd.Series],\n",
        "               y_series: Union[np.ndarray, pd.Series],\n",
        "               lag: int,\n",
        "               shuffle: bool = False,\n",
        "               n_shuffles: int = 300,\n",
        "               twoway: bool = False,\n",
        "               n_bins: int = 5,\n",
        "               discretization_method: str = 'quantile',\n",
        "               base: float = 2):\n",
        "\n",
        "      \"\"\"\n",
        "      Parameters:\n",
        "      mode (str): 'cumulative' or 'individual'\n",
        "      x_series: Input series X\n",
        "      y_series: Input series Y\n",
        "      lag: Number of lags (cumulative) or specific lag (individual)\n",
        "      shuffle (bool): Whether to perform shuffle test\n",
        "      n_shuffles (int): Number of shuffles if shuffle=True\n",
        "      twoway (bool): Whether to calculate bidirectional TE\n",
        "      n_bins: Number of bins for discretization\n",
        "      discretization_method: Method for discretization ('quantile' or 'uniform')\n",
        "      base: Base for logarithm in TE calculation\n",
        "      \"\"\"\n",
        "\n",
        "      self.mode = mode.lower()\n",
        "      self.lag = lag\n",
        "      self.do_shuffle = shuffle\n",
        "      self.n_shuffles = n_shuffles\n",
        "      self.twoway = twoway\n",
        "      self.base = base\n",
        "      self.dm = discretization_method\n",
        "      self.n_bins = n_bins\n",
        "\n",
        "      # Preprocess data\n",
        "      preprocessor = Preprocessing(n_bins = n_bins)\n",
        "      self.x_discrete = preprocessor.discretize_returns(x_series, method = discretization_method)\n",
        "      self.y_discrete = preprocessor.discretize_returns(y_series, method = discretization_method)\n",
        "\n",
        "      # Initialize appropriate calculator\n",
        "      self._initialize_calculator()\n",
        "\n",
        "      # Data validation\n",
        "      self._validate_inputs(mode, lag, discretization_method)\n",
        "\n",
        "  def _validate_inputs(self, mode: str, lag: int, disc_method: str) -> None:\n",
        "    \"\"\"Validate input parameters\"\"\"\n",
        "    if mode.lower() not in ['cumulative', 'individual']:\n",
        "        raise ValueError(\"Mode must be either 'cumulative' or 'individual'\")\n",
        "    if lag < 1:\n",
        "        raise ValueError(\"Lag must be positive\")\n",
        "    if disc_method not in ['quantile', 'uniform', 'pass']:\n",
        "        raise ValueError(\"Discretization method must be 'quantile' or 'uniform'\")\n",
        "\n",
        "\n",
        "  def _initialize_calculator(self) -> None:\n",
        "    \"\"\"Initialize appropriate TE calculator based on mode\"\"\"\n",
        "\n",
        "    if self.mode == 'cumulative':\n",
        "        self.calculator = CumulativeTransferEntropy(\n",
        "            self.x_discrete,\n",
        "            self.y_discrete,\n",
        "            self.lag,\n",
        "            self.base\n",
        "        )\n",
        "    else:\n",
        "        self.calculator = IndividualTransferEntropy(\n",
        "            self.x_discrete,\n",
        "            self.y_discrete,\n",
        "            self.lag,\n",
        "            self.base\n",
        "        )\n",
        "\n",
        "  def fit(self) -> dict:\n",
        "    \"\"\"\n",
        "    Calculate Transfer Entropy based on initialization parameters.\n",
        "    Returns dict with requested calculations.\n",
        "    \"\"\"\n",
        "\n",
        "    if not self.twoway:\n",
        "        # One-way TE calculation\n",
        "        te_value = self.calculator.processor()\n",
        "\n",
        "        if not self.do_shuffle:\n",
        "            self._results = {'te_value': te_value}\n",
        "        else:\n",
        "            # With shuffle test\n",
        "            shuffle_test = ShuffleTest(\n",
        "                self.calculator,\n",
        "                self.mode,\n",
        "                self.x_discrete,\n",
        "                self.y_discrete,\n",
        "                self.lag,\n",
        "                self.n_shuffles\n",
        "            )\n",
        "\n",
        "            shuffles, p_value = shuffle_test.perform_parallel_test()\n",
        "\n",
        "            self._results = {\n",
        "                'te_value': te_value,\n",
        "                'p_value': p_value,\n",
        "                'shuffles': shuffles,\n",
        "                'significant': p_value < 0.05\n",
        "            }\n",
        "    else:\n",
        "        # Two-way TE calculation\n",
        "        # Forward direction (X -> Y)\n",
        "        te_forward = self.calculator.processor()\n",
        "\n",
        "        # Reverse direction (Y -> X)\n",
        "        reverse_calculator = TransferEntropy(\n",
        "            self.mode,\n",
        "            self.y_discrete,\n",
        "            self.x_discrete,\n",
        "            self.lag,\n",
        "            shuffle=self.do_shuffle,\n",
        "            n_shuffles=self.n_shuffles,\n",
        "            twoway=False,  # Prevent infinite recursion\n",
        "        )\n",
        "\n",
        "        reverse_results = reverse_calculator.fit()\n",
        "\n",
        "        if not self.do_shuffle:\n",
        "            self._results = {\n",
        "                'x->y': {'te_value': te_forward},\n",
        "                'y->x': {'te_value': reverse_results['te_value']}\n",
        "            }\n",
        "        else:\n",
        "            # With shuffle tests for both directions\n",
        "            forward_test = ShuffleTest(\n",
        "                self.calculator,\n",
        "                self.mode,\n",
        "                self.x_discrete,\n",
        "                self.y_discrete,\n",
        "                self.lag,\n",
        "                self.n_shuffles\n",
        "            )\n",
        "\n",
        "            fwd_shuffles, fwd_p_value = forward_test.perform_parallel_test()\n",
        "\n",
        "            self._results = {\n",
        "                'x->y': {\n",
        "                    'te_value': te_forward,\n",
        "                    'p_value': fwd_p_value,\n",
        "                    'shuffles': fwd_shuffles,\n",
        "                    'significant': fwd_p_value < 0.05\n",
        "                },\n",
        "                'y->x': reverse_results\n",
        "            }\n",
        "\n",
        "    return self._results\n",
        "\n",
        "  @property\n",
        "  def results(self) -> dict:\n",
        "    \"\"\"Get latest calculation results\"\"\"\n",
        "    if not hasattr(self, '_results'):\n",
        "        return None\n",
        "    return self._results\n",
        "\n",
        "  @property\n",
        "  def calculator_type(self) -> str:\n",
        "    \"\"\"Get type of TE calculator being used\"\"\"\n",
        "    return self.calculator.__class__.__name__"
      ],
      "metadata": {
        "id": "_yNYhyWLNy6f"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IndividualTransferEntropy:\n",
        "\n",
        "  def __init__(self,\n",
        "               x_series: Union[np.ndarray, pd.Series],\n",
        "               y_series: Union[np.ndarray, pd.Series],\n",
        "               lag: int,\n",
        "               n_bins: int = 5,\n",
        "               base: int = 2\n",
        "               ):\n",
        "    \"\"\"\n",
        "    Initialize the TransferEntropyCalculator.\n",
        "\n",
        "    Parameters:\n",
        "    n_bins (int): Number of bins to use for discretization. Default is 5.\n",
        "    discretization_method (str): Method to use for discretization.\n",
        "                                  Options are 'quantile' or 'uniform'. Default is 'quantile'.\n",
        "    base (float): The logarithm base to use for entropy calculations. Default is 2 (bits).\n",
        "\n",
        "    Attributes:\n",
        "    n_bins (int): Number of bins for discretization.\n",
        "    discretization_method (str): Method used for discretization.\n",
        "    base (float): Logarithm base for entropy calculations.\n",
        "    \"\"\"\n",
        "    self.n_bins = n_bins\n",
        "    self.base = base\n",
        "\n",
        "    self.lag = lag\n",
        "\n",
        "    self.x_discrete = x_series\n",
        "    self.y_discrete = y_series\n",
        "\n",
        "  def processor(self):\n",
        "    joint_probs = self.calculate_joint_probability(self.y_discrete, self.x_discrete, self.lag)\n",
        "    cond_probs_x = self.calculate_conditional_probability_x(self.y_discrete, self.lag)\n",
        "    cond_probs_xy = self.calculate_conditional_probability_xy(self.y_discrete, self.x_discrete, self.lag)\n",
        "    self.te_result = self.calculate_transfer_entropy(joint_probs, cond_probs_x, cond_probs_xy)\n",
        "    return self.te_result\n",
        "\n",
        "  def calculate_joint_probability(self, x, y, lag):\n",
        "      n = len(x)\n",
        "      joint_states = []\n",
        "      for t in range(lag, n - 1):\n",
        "          x_future = x[t + 1]\n",
        "          x_past = x[t - lag + 1]  # Only consider the specific lag\n",
        "          y_past = y[t - lag + 1]  # Only consider the specific lag\n",
        "          joint_state = (x_future, x_past, y_past)\n",
        "          joint_states.append(joint_state)\n",
        "      state_counts = Counter(joint_states)\n",
        "      total_counts = sum(state_counts.values())\n",
        "      return {state: count / total_counts for state, count in state_counts.items()}\n",
        "\n",
        "  def calculate_conditional_probability_x(self, x, lag):\n",
        "      n = len(x)\n",
        "      state_counts = Counter()\n",
        "      next_state_counts = Counter()\n",
        "      for t in range(lag, n - 1):\n",
        "          current_state = x[t - lag + 1]  # Only consider the specific lag\n",
        "          next_state = x[t + 1]\n",
        "          state_counts[current_state] += 1\n",
        "          next_state_counts[(next_state, current_state)] += 1\n",
        "\n",
        "      conditional_probs = {}\n",
        "      for (next_state, current_state), count in next_state_counts.items():\n",
        "          if current_state in state_counts:\n",
        "              conditional_probs[(next_state, current_state)] = count / state_counts[current_state]\n",
        "\n",
        "      return conditional_probs\n",
        "\n",
        "  def calculate_conditional_probability_xy(self, x, y, lag):\n",
        "      n = len(x)\n",
        "      state_counts = Counter()\n",
        "      next_state_counts = Counter()\n",
        "      for t in range(lag, n - 1):\n",
        "          current_state_x = x[t - lag + 1]  # Only consider the specific lag\n",
        "          current_state_y = y[t - lag + 1]  # Only consider the specific lag\n",
        "          current_state = (current_state_x, current_state_y)\n",
        "          next_state = x[t + 1]\n",
        "          state_counts[current_state] += 1\n",
        "          next_state_counts[(next_state,) + current_state] += 1\n",
        "\n",
        "      conditional_probs = {}\n",
        "      for (next_state, *current_state), count in next_state_counts.items():\n",
        "          current_state = tuple(current_state)\n",
        "          if current_state in state_counts:\n",
        "              conditional_probs[(next_state, current_state)] = count / state_counts[current_state]\n",
        "\n",
        "      return conditional_probs\n",
        "\n",
        "  def calculate_transfer_entropy(self, joint_probs, cond_probs_x, cond_probs_xy):\n",
        "      te = 0\n",
        "      for (x_next, x_past, y_past), joint_prob in joint_probs.items():\n",
        "          if (x_next, x_past) in cond_probs_x and (x_next, (x_past, y_past)) in cond_probs_xy:\n",
        "              p_x = cond_probs_x[(x_next, x_past)]\n",
        "              p_xy = cond_probs_xy[(x_next, (x_past, y_past))]\n",
        "              te += joint_prob * np.log2(p_xy / p_x)\n",
        "      return te"
      ],
      "metadata": {
        "id": "ubnN69SxmSQy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CumulativeTransferEntropy:\n",
        "\n",
        "  def __init__(self,\n",
        "               x_series: Union[np.ndarray, pd.Series],\n",
        "               y_series: Union[np.ndarray, pd.Series],\n",
        "               lag: int,\n",
        "               n_bins: int = 5,\n",
        "               base: int = 2):\n",
        "\n",
        "    \"\"\"\n",
        "    Initialize the TransferEntropyCalculator.\n",
        "\n",
        "    Parameters:\n",
        "    n_bins (int): Number of bins to use for discretization. Default is 5.\n",
        "    discretization_method (str): Method to use for discretization.\n",
        "                                  Options are 'quantile' or 'uniform'. Default is 'quantile'.\n",
        "    base (float): The logarithm base to use for entropy calculations. Default is 2 (bits).\n",
        "\n",
        "    Attributes:\n",
        "    n_bins (int): Number of bins for discretization.\n",
        "    discretization_method (str): Method used for discretization.\n",
        "    base (float): Logarithm base for entropy calculations.\n",
        "    \"\"\"\n",
        "\n",
        "    self.n_bins = n_bins\n",
        "    self.base = base\n",
        "\n",
        "    self.x_discrete = x_series\n",
        "    self.y_discrete = y_series\n",
        "\n",
        "    self.lag = lag\n",
        "    self.te = None\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"TransferEntropyCalculator(n_bins={self.n_bins}, \" \\\n",
        "            f\"base={self.base})\"\n",
        "\n",
        "  def processor(self):\n",
        "    joint_probs = self.calculate_joint_probability(self.y_discrete, self.x_discrete, self.lag, self.lag)\n",
        "    cond_probs_x = self.calculate_conditional_probability_x(self.y_discrete, self.lag)\n",
        "    cond_probs_xy = self.calculate_conditional_probability_xy(self.y_discrete, self.x_discrete, self.lag, self.lag)\n",
        "    self.te_result = self.calculate_transfer_entropy(joint_probs, cond_probs_x, cond_probs_xy, self.lag)\n",
        "    return self.te_result\n",
        "\n",
        "  def calculate_joint_probability(self, x, y, k, l):\n",
        "      n = len(x)\n",
        "      joint_states = []\n",
        "      for t in range(max(k, l), n - 1):\n",
        "          x_future = x[t + 1]\n",
        "          x_past = tuple(x[t - i] for i in range(k))\n",
        "          y_past = tuple(y[t - i] for i in range(l))\n",
        "          joint_state = (x_future,) + x_past + y_past\n",
        "          joint_states.append(joint_state)\n",
        "      state_counts = Counter(joint_states)\n",
        "      total_counts = sum(state_counts.values())\n",
        "      return {state: count / total_counts for state, count in state_counts.items()}\n",
        "\n",
        "  def calculate_conditional_probability_x(self, x, k):\n",
        "      n = len(x)\n",
        "      state_counts = Counter()\n",
        "      next_state_counts = Counter()\n",
        "      for t in range(k, n - 1):\n",
        "          current_state = tuple(x[t - i] for i in range(k))\n",
        "          next_state = x[t + 1]\n",
        "          state_counts[current_state] += 1\n",
        "          next_state_counts[(next_state,) + current_state] += 1\n",
        "\n",
        "      conditional_probs = {}\n",
        "      for full_state, count in next_state_counts.items():\n",
        "          next_state = full_state[0]\n",
        "          current_state = full_state[1:]\n",
        "          if current_state in state_counts:\n",
        "              conditional_probs[(next_state, current_state)] = count / state_counts[current_state]\n",
        "\n",
        "      return conditional_probs\n",
        "\n",
        "  def calculate_conditional_probability_xy(self, x, y, k, l):\n",
        "      n = len(x)\n",
        "      state_counts = Counter()\n",
        "      next_state_counts = Counter()\n",
        "      for t in range(max(k, l), n - 1):\n",
        "          current_state_x = tuple(x[t - i] for i in range(k))\n",
        "          current_state_y = tuple(y[t - i] for i in range(l))\n",
        "          current_state = current_state_x + current_state_y\n",
        "          next_state = x[t + 1]\n",
        "          state_counts[current_state] += 1\n",
        "          next_state_counts[(next_state,) + current_state] += 1\n",
        "\n",
        "      conditional_probs = {}\n",
        "      for full_state, count in next_state_counts.items():\n",
        "          next_state = full_state[0]\n",
        "          current_state = full_state[1:]\n",
        "          if current_state in state_counts:\n",
        "              conditional_probs[(next_state, current_state)] = count / state_counts[current_state]\n",
        "\n",
        "      return conditional_probs\n",
        "\n",
        "  def calculate_transfer_entropy(self, joint_probs, cond_probs_x, cond_probs_xy, lag):\n",
        "      te = 0\n",
        "      for (x_next, *state), joint_prob in joint_probs.items():\n",
        "          state = tuple(state)\n",
        "          x_state = state[:lag]\n",
        "          xy_state = state\n",
        "          if (x_next, x_state) in cond_probs_x and (x_next, xy_state) in cond_probs_xy:\n",
        "              p_x = cond_probs_x[(x_next, x_state)]\n",
        "              p_xy = cond_probs_xy[(x_next, xy_state)]\n",
        "              te += joint_prob * np.log2(p_xy / p_x)\n",
        "      return te\n",
        "\n",
        "  def shuffle_series(self, series):\n",
        "      return pd.Series(np.random.permutation(series.values), index=series.index)\n"
      ],
      "metadata": {
        "id": "_TtKJHiomYtU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransferEntropyNetwork:\n",
        "  def __init__(self,\n",
        "               df: pd.DataFrame,\n",
        "               mode: str = 'individual',\n",
        "               lag: int = 1,\n",
        "               shuffle: bool = True,  # Default True as we need p-values for thresholding\n",
        "               n_shuffles: int = 300,\n",
        "               significance_threshold: float = 0.05):\n",
        "      \"\"\"\n",
        "      Parameters:\n",
        "      df: Input dataframe where columns are variables to analyze\n",
        "      mode: Type of TE calculation ('individual' or 'cumulative')\n",
        "      lag: Lag for TE calculation\n",
        "      shuffle: Whether to perform shuffle test\n",
        "      n_shuffles: Number of shuffles for significance testing\n",
        "      significance_threshold: p-value threshold for edge inclusion\n",
        "      \"\"\"\n",
        "\n",
        "      self.df = df\n",
        "      self.mode = mode.lower()\n",
        "      self.lag = lag\n",
        "      self.shuffle = shuffle\n",
        "      self.n_shuffles = n_shuffles\n",
        "      self.significance_threshold = significance_threshold\n",
        "      self.names = df.columns\n",
        "\n",
        "      self._te_frame = None\n",
        "      self._te_matrix = None\n",
        "\n",
        "  def calculate_te_frame(self) -> pd.DataFrame:\n",
        "      \"\"\"Calculate TE for all pairs of variables\"\"\"\n",
        "\n",
        "      # Generate all pairs excluding self-pairs\n",
        "      pairs = itertools.product(self.names, self.names)\n",
        "      pairs = [pair for pair in pairs if pair[0] != pair[1]]\n",
        "\n",
        "      # Create results DataFrame\n",
        "      results = pd.DataFrame(\n",
        "          columns=pd.MultiIndex.from_product(\n",
        "              [['direct', 'reverse'], ['entropy', 'pval']],\n",
        "              names=['direction', 'value']\n",
        "          )\n",
        "      )\n",
        "\n",
        "      # Calculate TE for each pair\n",
        "      for (elem1, elem2) in tqdm(pairs, desc='Calculating Transfer Entropy'):\n",
        "          x = self.df.loc[:, elem1]\n",
        "          y = self.df.loc[:, elem2]\n",
        "\n",
        "          # Calculate TE using our new TransferEntropy class\n",
        "          model = TransferEntropy(\n",
        "              mode=self.mode,\n",
        "              x_series=x,\n",
        "              y_series=y,\n",
        "              lag=self.lag,\n",
        "              shuffle=self.shuffle,\n",
        "              n_shuffles=self.n_shuffles,\n",
        "              twoway=True  # We need both directions\n",
        "          )\n",
        "\n",
        "          result = model.fit()\n",
        "\n",
        "          key = f'{elem1} -> {elem2}'\n",
        "          results.loc[key] = [\n",
        "              result['x_to_y']['te_value'],\n",
        "              result['x_to_y'].get('p_value', None),\n",
        "              result['y_to_x']['te_value'],\n",
        "              result['y_to_x'].get('p_value', None)\n",
        "          ]\n",
        "\n",
        "      self._te_frame = results\n",
        "      return results\n",
        "\n",
        "  def generate_matrix(self) -> pd.DataFrame:\n",
        "      \"\"\"Generate square matrix of significant TE values\"\"\"\n",
        "      if self._te_frame is None:\n",
        "          self.calculate_te_frame()\n",
        "\n",
        "      n = len(self.names)\n",
        "      matrix = np.zeros((n, n))\n",
        "\n",
        "      # Fill the matrix\n",
        "      for i, elem1 in enumerate(self.names):\n",
        "          for j, elem2 in enumerate(self.names):\n",
        "              if i != j:\n",
        "                  forward_key = f'{elem1} -> {elem2}'\n",
        "                  reverse_key = f'{elem2} -> {elem1}'\n",
        "\n",
        "                  if forward_key in self._te_frame.index:\n",
        "                      if not self.shuffle or self._te_frame.loc[forward_key, ('direct', 'pval')] < self.significance_threshold:\n",
        "                          matrix[i, j] = self._te_frame.loc[forward_key, ('direct', 'entropy')]\n",
        "                  elif reverse_key in self._te_frame.index:\n",
        "                      if not self.shuffle or self._te_frame.loc[reverse_key, ('reverse', 'pval')] < self.significance_threshold:\n",
        "                          matrix[i, j] = self._te_frame.loc[reverse_key, ('reverse', 'entropy')]\n",
        "\n",
        "      self._te_matrix = pd.DataFrame(matrix, index=self.names, columns=self.names)\n",
        "      return self._te_matrix.round(3)\n",
        "\n",
        "  def plot_network(self) -> None:\n",
        "      \"\"\"Generate network visualization using existing TransferEntropyGraph class\"\"\"\n",
        "      if self._te_matrix is None:\n",
        "          self.generate_matrix()\n",
        "\n",
        "      TransferEntropyGraph(self._te_matrix.values, list(self.names))\n",
        "\n",
        "  @property\n",
        "  def te_frame(self) -> pd.DataFrame:\n",
        "      \"\"\"Access TE calculation results\"\"\"\n",
        "      return self._te_frame\n",
        "\n",
        "  @property\n",
        "  def te_matrix(self) -> pd.DataFrame:\n",
        "      \"\"\"Access TE matrix\"\"\"\n",
        "      return self._te_matrix"
      ],
      "metadata": {
        "id": "GwYuqwP_Ny3_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransferEntropyGraph:\n",
        "\n",
        "  def __init__(self, matrix: np.array,\n",
        "                names: list) -> None:\n",
        "\n",
        "    # make sure that a square matrix is passed into the constructor\n",
        "    assert matrix.shape[0] == matrix.shape[1]\n",
        "\n",
        "    self.data = matrix\n",
        "    self.names = names\n",
        "    self.visualise_transfer_entropy(self.data, self.names)\n",
        "\n",
        "  def find_most_influential_node(self,\n",
        "                                  matrix):\n",
        "\n",
        "    outgoing_sum = np.sum(matrix, axis=1)\n",
        "    return np.argmax(outgoing_sum)\n",
        "\n",
        "  def draw_network_with_arrows(self,\n",
        "                                G,\n",
        "                                pos,\n",
        "                                ax,\n",
        "                                edge_weights):\n",
        "\n",
        "    for edge in G.edges(data=True):\n",
        "        start = pos[edge[0]]\n",
        "        end = pos[edge[1]]\n",
        "        weight = edge[2]['weight']\n",
        "\n",
        "        # Calculate the direction vector\n",
        "        dir_vec = np.array(end) - np.array(start)\n",
        "        dir_vec /= np.linalg.norm(dir_vec)\n",
        "\n",
        "        # Shorten the edge to prevent overlap with nodes\n",
        "        start = np.array(start) + dir_vec * 0.15\n",
        "        end = np.array(end) - dir_vec * 0.15\n",
        "\n",
        "        # Draw the edge\n",
        "        arrow = FancyArrowPatch(start, end,\n",
        "                              arrowstyle='-|>',\n",
        "                              color='gray',\n",
        "                              mutation_scale=10,  # Size of arrowhead\n",
        "                              linewidth=edge_weights[edge[:2]] * 10,\n",
        "                              shrinkA=0, shrinkB=0)\n",
        "        ax.add_patch(arrow)\n",
        "\n",
        "  def visualise_transfer_entropy(self,\n",
        "                                  matrix,\n",
        "                                  names,\n",
        "                                  threshold=0) -> None:\n",
        "\n",
        "    G = nx.DiGraph()\n",
        "    G.add_nodes_from(names)\n",
        "\n",
        "    for i, row_name in enumerate(names):\n",
        "        for j, col_name in enumerate(names):\n",
        "            if i != j and matrix[i, j] > threshold:\n",
        "                G.add_edge(row_name, col_name, weight=matrix[i, j])\n",
        "\n",
        "    influential_index = self.find_most_influential_node(matrix)\n",
        "    influential_node = names[influential_index]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    pos = nx.spring_layout(G, k=0.9)\n",
        "\n",
        "    # Draw nodes\n",
        "    node_colors = ['red' if node == influential_node else 'lightblue' for node in G.nodes()]\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=700, node_color=node_colors, ax=ax)\n",
        "    nx.draw_networkx_labels(G, pos, font_size=10, font_weight=\"bold\", ax=ax)\n",
        "\n",
        "    # Draw edges with arrows\n",
        "    edge_weights = nx.get_edge_attributes(G, 'weight')\n",
        "    self.draw_network_with_arrows(G, pos, ax, edge_weights)\n",
        "\n",
        "    # Add edge labels\n",
        "    edge_labels = {(u, v): f'{d[\"weight\"]:.2f}' for (u, v, d) in G.edges(data=True)}\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8, ax=ax)\n",
        "\n",
        "    plt.title(\"Transfer Entropy Network\", fontsize=16)\n",
        "    ax.set_axis_off()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "    plt.savefig('transfer_entropy_network.png', dpi=300, bbox_inches='tight')\n",
        "    # plt.close()\n",
        "\n",
        "    print(f\"Transfer entropy network visualization has been saved as 'transfer_entropy_network.png'.\")\n",
        "    print(f\"The most influential node is: {influential_node}\")"
      ],
      "metadata": {
        "id": "-IWch1ibNy1s"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_series = pd.Series([1, 0, 0, 1, 0, 1, 1, 1, 0, 0])\n",
        "y_series = pd.Series([1, 1, 0, 1, 1, 0, 1, 0, 1, 0])"
      ],
      "metadata": {
        "id": "izDpIal0Hv55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = TestDataGenerator(n_samples = 5_000)\n",
        "x_series, y_series = generator.complexData()\n",
        "df = pd.DataFrame({'x':x_series, 'y':y_series})"
      ],
      "metadata": {
        "id": "A4ZSFEURNyxI"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "te = TransferEntropy(mode = 'individual', discretization_method='uniform', n_bins = 5, x_series = x_series, y_series = y_series, lag = 1, twoway = True, shuffle = True)"
      ],
      "metadata": {
        "id": "Itgx5Ud9bdlY"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = te.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlSr-soubh8D",
        "outputId": "f1a6cde6-7c33-4a68-f237-9703230d9d3d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running shuffle iterations: 100%|██████████| 300/300 [00:33<00:00,  9.00it/s]\n",
            "Running shuffle iterations: 100%|██████████| 300/300 [00:31<00:00,  9.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res['x->y']['te_value'], res['y->x']['te_value'], res['x->y']['p_value'], res['y->x']['p_value']"
      ],
      "metadata": {
        "id": "yF7-HppfOo9d",
        "outputId": "fcbf80f3-2120-4155-8d8a-7b031d4c06b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.046064213702571744, 0.0011628687208891848, 0.0, 0.02666666666666667)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize network analyzer\n",
        "te_network = TransferEntropyNetwork(\n",
        "    df=df,\n",
        "    mode='individual',\n",
        "    lag=1,\n",
        "    shuffle=True,\n",
        "    n_shuffles=300\n",
        ")\n",
        "\n",
        "# Calculate and visualize\n",
        "te_network.calculate_te_frame()  # Get detailed results\n",
        "te_network.generate_matrix()     # Get matrix representation\n",
        "te_network.plot_network()        # Visualize network"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "AlDx_k0qNyuP",
        "outputId": "137b3a55-f251-40b0-9688-12d795673cff"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running shuffle iterations: 100%|██████████| 300/300 [00:37<00:00,  7.93it/s]\n",
            "Running shuffle iterations: 100%|██████████| 300/300 [00:38<00:00,  7.76it/s]\n",
            "Calculating Transfer Entropy:   0%|          | 0/2 [01:17<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'x_to_y'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-980908c27715>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Calculate and visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mte_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_te_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get detailed results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mte_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Get matrix representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mte_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# Visualize network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-3fcba80b3899>\u001b[0m in \u001b[0;36mcalculate_te_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m           \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{elem1} -> {elem2}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m           results.loc[key] = [\n\u001b[0;32m---> 65\u001b[0;31m               \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_to_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'te_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m               \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_to_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p_value'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m               \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_to_x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'te_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'x_to_y'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.corrcoef(x_series[1:], y_series[:-1])"
      ],
      "metadata": {
        "id": "1tRp0mXuGNw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a730b4-20c9-473c-8e05-e49372f14036"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.        , -0.00187451],\n",
              "       [-0.00187451,  1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.corrcoef(x_series[:-1], y_series[1:])"
      ],
      "metadata": {
        "id": "cgbzFWr_GNuI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "874d4a51-ec6f-4250-e391-f1f67fcb510b"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.26706441],\n",
              "       [0.26706441, 1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bewox6cxGNr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-3DZDAq5GNph"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}