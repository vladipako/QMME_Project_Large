{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE9dwfXn4Ao5sYaaZYDdz3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladipako/Transfer_Entropy_Project/blob/main/TE_Package.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from typing import Union\n",
        "from tqdm import tqdm\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "mXasH-XswIhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# analysis = TransferEntropy('cumulative', x_series, y_series, x_lag = 1)\n",
        "# result = analysis.te_calculator.processor()\n",
        "# print(result)"
      ],
      "metadata": {
        "id": "dmek3LNtpjvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Utilities:\n",
        "  def __init__(self, entropy, mode, x_series, y_series, lag, n_shuffles):\n",
        "    self.entropy = entropy\n",
        "    self.x_series = x_series\n",
        "    self.y_series = y_series\n",
        "    self.mode = mode\n",
        "    self.n_shuffles = n_shuffles\n",
        "    self.lag = lag\n",
        "    self.entropy = entropy\n",
        "\n",
        "  def shuffle_series(self, series):\n",
        "    return np.random.permutation(series)\n",
        "\n",
        "  def perform_shuffle_test(self):\n",
        "    shuffle_tes = []\n",
        "    for _ in tqdm(range(self.n_shuffles), desc='Running through shuffle iterations: '):\n",
        "        y_shuffled = self.shuffle_series(self.y_series)\n",
        "        analysis = TransferEntropy(self.mode, self.x_series, y_shuffled, self.lag)\n",
        "        shuffle_te = analysis.te_calculator.processor()\n",
        "        shuffle_tes.append(shuffle_te)\n",
        "\n",
        "    p_value = sum(shuffle_te >= self.entropy for shuffle_te in shuffle_tes) / self.n_shuffles\n",
        "    return shuffle_tes, p_value\n",
        "\n",
        "  def shuffle_and_calculate(self, y_shuffled):\n",
        "    analysis = TransferEntropy(self.mode, self.x_series, y_shuffled, self.lag)\n",
        "    return analysis.te_calculator.processor()\n",
        "\n",
        "  def perform_shuffle_test_parallel(self):\n",
        "    shuffle_func = partial(self.shuffle_and_calculate)\n",
        "    with Pool(processes=cpu_count()) as pool:\n",
        "\n",
        "        shuffle_tes = list(tqdm(\n",
        "            pool.imap(shuffle_func, (self.shuffle_series(self.y_series) for _ in range(self.n_shuffles))),\n",
        "            total=self.n_shuffles,\n",
        "            desc='Running through shuffle iterations'\n",
        "        ))\n",
        "\n",
        "    p_value = sum(shuffle_te >= self.entropy for shuffle_te in shuffle_tes) / self.n_shuffles\n",
        "    return shuffle_tes, p_value"
      ],
      "metadata": {
        "id": "PnZEPtcGp_jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataGenerator:\n",
        "\n",
        "  def __init__(self,\n",
        "               n_samples = 10_000,\n",
        "               x_scale = 1,\n",
        "               y_scale = 1,\n",
        "               ):\n",
        "\n",
        "    self.n_samples = n_samples\n",
        "    self.x_scale = x_scale\n",
        "    self.y_scale = y_scale\n",
        "\n",
        "  def randomData(self):\n",
        "    x = np.random.normal(loc = 0, scale = self.x_scale, size = self.n_samples)\n",
        "    y = np.random.normal(loc = 0, scale = self.y_scale, size = self.n_samples)\n",
        "    return pd.Series(x), pd.Series(y)\n",
        "\n",
        "  def simpleData(self):\n",
        "    x = np.random.normal(loc = 0, scale = self.x_scale, size = self.n_samples)\n",
        "    x_scaled = x * np.pi\n",
        "    y = np.sin(np.roll(x_scaled, 1)) + np.random.normal(loc = 0, scale = self.y_scale, size = self.n_samples)\n",
        "    y[0] = np.sin(x_scaled[-1]) + np.random.randn() * self.y_scale\n",
        "    return pd.Series(x), pd.Series(y)\n",
        "\n",
        "  def complexData(self):\n",
        "    x = np.random.normal(loc = 0, scale = self.x_scale, size = self.n_samples)\n",
        "    y = np.zeros(self.n_samples)\n",
        "\n",
        "    # Initialize the first two y values (as they depend on non-existent past x values)\n",
        "    y[0] = np.random.randn() * self.y_scale\n",
        "    y[1] = np.random.randn() * self.y_scale\n",
        "\n",
        "    for t in range(2, self.n_samples):\n",
        "      if x[t] > 0:\n",
        "          y[t] = np.sin(x[t-1]) + np.random.randn() * self.y_scale\n",
        "      else:\n",
        "          y[t] = np.cos(x[t-2])**2 + np.random.randn() * self.y_scale\n",
        "\n",
        "    return pd.Series(x), pd.Series(y)"
      ],
      "metadata": {
        "id": "NHZc9mWndQOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransferEntropy:\n",
        "\n",
        "  def __init__(self,\n",
        "               mode: str,\n",
        "               x_series: Union[np.ndarray, pd.Series],\n",
        "               y_series: Union[np.ndarray, pd.Series],\n",
        "               x_lag: int,\n",
        "               discretization_method: str = 'quantile',\n",
        "               n_bins: int = 5,\n",
        "               base: float = 2,\n",
        "               ):\n",
        "    \"\"\"\n",
        "    Initialize the TransferEntropyAnalyzer.\n",
        "\n",
        "    Parameters:\n",
        "    mode (str): 'cumulative' or 'individual'\n",
        "    x_series (array-like): Input series X\n",
        "    y_series (array-like): Input series Y\n",
        "    x_lag (int): number of lags (cumulative case) or specific lag (individual case)\n",
        "    n_bins (int): Number of bins for discretization\n",
        "    discretization_method (str): 'quantile' or 'uniform'\n",
        "    base (float): Base for logarithm in TE calculation\n",
        "    \"\"\"\n",
        "\n",
        "    self.mode = mode.lower()\n",
        "    self.x_lag = x_lag\n",
        "    self.n_bins = n_bins\n",
        "    self.discretization_method = discretization_method\n",
        "    self.base = base\n",
        "    self.x_series = self.discretize_returns(x_series)\n",
        "    self.y_series = self.discretize_returns(y_series)\n",
        "\n",
        "    if self.mode == 'cumulative':\n",
        "        self.te_calculator = CumulativeTransferEntropy(\n",
        "            self.x_series, self.y_series, self.x_lag, self.n_bins, self.base\n",
        "            )\n",
        "\n",
        "    elif self.mode == 'individual':\n",
        "        self.te_calculator = IndividualTransferEntropy(\n",
        "            self.x_series, self.y_series, self.x_lag, self.n_bins, self.base\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Mode must be either 'cumulative' or 'individual'\")\n",
        "\n",
        "    self.results = None\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'TransferEntropyAnalyzer(mode={self.mode}'\n",
        "\n",
        "  def shuffleTest(self, size):\n",
        "    entropy = self.te_calculator.processor()\n",
        "    utils = Utilities(entropy, self.mode, self.x_series, self.y_series, self.x_lag, size)\n",
        "    # return utils.perform_shuffle_test()\n",
        "    return utils.perform_shuffle_test_parallel()\n",
        "\n",
        "  def calculate(self) -> float:\n",
        "    return self.te_calculator.processor()\n",
        "\n",
        "  def discretize_returns(self, returns: Union[np.ndarray, pd.Series]) -> pd.Series:\n",
        "\n",
        "    if self.discretization_method == 'uniform':\n",
        "        bins = np.linspace(np.min(returns), np.max(returns), self.n_bins + 1)\n",
        "        discretized = pd.cut(returns, bins=self.bins, labels=False, include_lowest=True)\n",
        "    elif self.discretization_method == 'quantile':\n",
        "        discretized = pd.qcut(returns, q=self.n_bins, labels=False, duplicates='drop')\n",
        "    else:\n",
        "        raise ValueError(\"Discretization method must be either 'uniform' or 'quantile'\")\n",
        "\n",
        "    return pd.Series(discretized, index=getattr(returns, 'index', None))\n"
      ],
      "metadata": {
        "id": "6hmmSQGMnjXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IndividualTransferEntropy:\n",
        "\n",
        "\n",
        "  def __init__(self,\n",
        "               x_series: Union[np.ndarray, pd.Series],\n",
        "               y_series: Union[np.ndarray, pd.Series],\n",
        "               lag: int,\n",
        "               n_bins: int = 5,\n",
        "               base: int = 2\n",
        "               ):\n",
        "    \"\"\"\n",
        "    Initialize the TransferEntropyCalculator.\n",
        "\n",
        "    Parameters:\n",
        "    n_bins (int): Number of bins to use for discretization. Default is 5.\n",
        "    discretization_method (str): Method to use for discretization.\n",
        "                                  Options are 'quantile' or 'uniform'. Default is 'quantile'.\n",
        "    base (float): The logarithm base to use for entropy calculations. Default is 2 (bits).\n",
        "\n",
        "    Attributes:\n",
        "    n_bins (int): Number of bins for discretization.\n",
        "    discretization_method (str): Method used for discretization.\n",
        "    base (float): Logarithm base for entropy calculations.\n",
        "    \"\"\"\n",
        "    self.n_bins = n_bins\n",
        "    self.base = base\n",
        "\n",
        "    self.lag = lag\n",
        "\n",
        "    self.x_discrete = x_series\n",
        "    self.y_discrete = y_series\n",
        "\n",
        "  def processor(self):\n",
        "    joint_probs = self.calculate_joint_probability(self.y_discrete, self.x_discrete, self.lag)\n",
        "    cond_probs_x = self.calculate_conditional_probability_x(self.y_discrete, self.lag)\n",
        "    cond_probs_xy = self.calculate_conditional_probability_xy(self.y_discrete, self.x_discrete, self.lag)\n",
        "    self.te_result = self.calculate_transfer_entropy(joint_probs, cond_probs_x, cond_probs_xy)\n",
        "    return self.te_result\n",
        "\n",
        "  def calculate_joint_probability(self, x, y, lag):\n",
        "      n = len(x)\n",
        "      joint_states = []\n",
        "      for t in range(lag, n - 1):\n",
        "          x_future = x[t + 1]\n",
        "          x_past = x[t - lag + 1]  # Only consider the specific lag\n",
        "          y_past = y[t - lag + 1]  # Only consider the specific lag\n",
        "          joint_state = (x_future, x_past, y_past)\n",
        "          joint_states.append(joint_state)\n",
        "      state_counts = Counter(joint_states)\n",
        "      total_counts = sum(state_counts.values())\n",
        "      return {state: count / total_counts for state, count in state_counts.items()}\n",
        "\n",
        "  def calculate_conditional_probability_x(self, x, lag):\n",
        "      n = len(x)\n",
        "      state_counts = Counter()\n",
        "      next_state_counts = Counter()\n",
        "      for t in range(lag, n - 1):\n",
        "          current_state = x[t - lag + 1]  # Only consider the specific lag\n",
        "          next_state = x[t + 1]\n",
        "          state_counts[current_state] += 1\n",
        "          next_state_counts[(next_state, current_state)] += 1\n",
        "\n",
        "      conditional_probs = {}\n",
        "      for (next_state, current_state), count in next_state_counts.items():\n",
        "          if current_state in state_counts:\n",
        "              conditional_probs[(next_state, current_state)] = count / state_counts[current_state]\n",
        "\n",
        "      return conditional_probs\n",
        "\n",
        "  def calculate_conditional_probability_xy(self, x, y, lag):\n",
        "      n = len(x)\n",
        "      state_counts = Counter()\n",
        "      next_state_counts = Counter()\n",
        "      for t in range(lag, n - 1):\n",
        "          current_state_x = x[t - lag + 1]  # Only consider the specific lag\n",
        "          current_state_y = y[t - lag + 1]  # Only consider the specific lag\n",
        "          current_state = (current_state_x, current_state_y)\n",
        "          next_state = x[t + 1]\n",
        "          state_counts[current_state] += 1\n",
        "          next_state_counts[(next_state,) + current_state] += 1\n",
        "\n",
        "      conditional_probs = {}\n",
        "      for (next_state, *current_state), count in next_state_counts.items():\n",
        "          current_state = tuple(current_state)\n",
        "          if current_state in state_counts:\n",
        "              conditional_probs[(next_state, current_state)] = count / state_counts[current_state]\n",
        "\n",
        "      return conditional_probs\n",
        "\n",
        "  def calculate_transfer_entropy(self, joint_probs, cond_probs_x, cond_probs_xy):\n",
        "      te = 0\n",
        "      for (x_next, x_past, y_past), joint_prob in joint_probs.items():\n",
        "          if (x_next, x_past) in cond_probs_x and (x_next, (x_past, y_past)) in cond_probs_xy:\n",
        "              p_x = cond_probs_x[(x_next, x_past)]\n",
        "              p_xy = cond_probs_xy[(x_next, (x_past, y_past))]\n",
        "              te += joint_prob * np.log2(p_xy / p_x)\n",
        "      return te"
      ],
      "metadata": {
        "id": "wtbiSlCSoIqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CumulativeTransferEntropy:\n",
        "\n",
        "  def __init__(self,\n",
        "               x_series: Union[np.ndarray, pd.Series],\n",
        "               y_series: Union[np.ndarray, pd.Series],\n",
        "               lag: int,\n",
        "               n_bins: int = 5,\n",
        "               base: int = 2):\n",
        "\n",
        "    \"\"\"\n",
        "    Initialize the TransferEntropyCalculator.\n",
        "\n",
        "    Parameters:\n",
        "    n_bins (int): Number of bins to use for discretization. Default is 5.\n",
        "    discretization_method (str): Method to use for discretization.\n",
        "                                  Options are 'quantile' or 'uniform'. Default is 'quantile'.\n",
        "    base (float): The logarithm base to use for entropy calculations. Default is 2 (bits).\n",
        "\n",
        "    Attributes:\n",
        "    n_bins (int): Number of bins for discretization.\n",
        "    discretization_method (str): Method used for discretization.\n",
        "    base (float): Logarithm base for entropy calculations.\n",
        "    \"\"\"\n",
        "\n",
        "    self.n_bins = n_bins\n",
        "    self.base = base\n",
        "\n",
        "    self.x_discrete = x_series\n",
        "    self.y_discrete = y_series\n",
        "\n",
        "    self.lag = lag\n",
        "    self.te = None\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"TransferEntropyCalculator(n_bins={self.n_bins}, \" \\\n",
        "            f\"base={self.base})\"\n",
        "\n",
        "  def processor(self):\n",
        "    joint_probs = self.calculate_joint_probability(self.y_discrete, self.x_discrete, self.lag, self.lag)\n",
        "    cond_probs_x = self.calculate_conditional_probability_x(self.y_discrete, self.lag)\n",
        "    cond_probs_xy = self.calculate_conditional_probability_xy(self.y_discrete, self.x_discrete, self.lag, self.lag)\n",
        "    self.te_result = self.calculate_transfer_entropy(joint_probs, cond_probs_x, cond_probs_xy, self.lag)\n",
        "    return self.te_result\n",
        "\n",
        "  def calculate_joint_probability(self, x, y, k, l):\n",
        "      n = len(x)\n",
        "      joint_states = []\n",
        "      for t in range(max(k, l), n - 1):\n",
        "          x_future = x[t + 1]\n",
        "          x_past = tuple(x[t - i] for i in range(k))\n",
        "          y_past = tuple(y[t - i] for i in range(l))\n",
        "          joint_state = (x_future,) + x_past + y_past\n",
        "          joint_states.append(joint_state)\n",
        "      state_counts = Counter(joint_states)\n",
        "      total_counts = sum(state_counts.values())\n",
        "      return {state: count / total_counts for state, count in state_counts.items()}\n",
        "\n",
        "  def calculate_conditional_probability_x(self, x, k):\n",
        "      n = len(x)\n",
        "      state_counts = Counter()\n",
        "      next_state_counts = Counter()\n",
        "      for t in range(k, n - 1):\n",
        "          current_state = tuple(x[t - i] for i in range(k))\n",
        "          next_state = x[t + 1]\n",
        "          state_counts[current_state] += 1\n",
        "          next_state_counts[(next_state,) + current_state] += 1\n",
        "\n",
        "      conditional_probs = {}\n",
        "      for full_state, count in next_state_counts.items():\n",
        "          next_state = full_state[0]\n",
        "          current_state = full_state[1:]\n",
        "          if current_state in state_counts:\n",
        "              conditional_probs[(next_state, current_state)] = count / state_counts[current_state]\n",
        "\n",
        "      return conditional_probs\n",
        "\n",
        "  def calculate_conditional_probability_xy(self, x, y, k, l):\n",
        "      n = len(x)\n",
        "      state_counts = Counter()\n",
        "      next_state_counts = Counter()\n",
        "      for t in range(max(k, l), n - 1):\n",
        "          current_state_x = tuple(x[t - i] for i in range(k))\n",
        "          current_state_y = tuple(y[t - i] for i in range(l))\n",
        "          current_state = current_state_x + current_state_y\n",
        "          next_state = x[t + 1]\n",
        "          state_counts[current_state] += 1\n",
        "          next_state_counts[(next_state,) + current_state] += 1\n",
        "\n",
        "      conditional_probs = {}\n",
        "      for full_state, count in next_state_counts.items():\n",
        "          next_state = full_state[0]\n",
        "          current_state = full_state[1:]\n",
        "          if current_state in state_counts:\n",
        "              conditional_probs[(next_state, current_state)] = count / state_counts[current_state]\n",
        "\n",
        "      return conditional_probs\n",
        "\n",
        "  def calculate_transfer_entropy(self, joint_probs, cond_probs_x, cond_probs_xy, lag):\n",
        "      te = 0\n",
        "      for (x_next, *state), joint_prob in joint_probs.items():\n",
        "          state = tuple(state)\n",
        "          x_state = state[:lag]\n",
        "          xy_state = state\n",
        "          if (x_next, x_state) in cond_probs_x and (x_next, xy_state) in cond_probs_xy:\n",
        "              p_x = cond_probs_x[(x_next, x_state)]\n",
        "              p_xy = cond_probs_xy[(x_next, xy_state)]\n",
        "              te += joint_prob * np.log2(p_xy / p_x)\n",
        "      return te\n",
        "\n",
        "  def shuffle_series(self, series):\n",
        "      return pd.Series(np.random.permutation(series.values), index=series.index)\n"
      ],
      "metadata": {
        "id": "1wr-BaslJnVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = TestDataGenerator(n_samples = 1_000)\n",
        "x_series, y_series = generator.complexData()\n",
        "\n",
        "analysis = TransferEntropy('individual', x_series, y_series, x_lag = 2)\n",
        "result = analysis.te_calculator.processor()\n",
        "\n",
        "te, p = analysis.shuffleTest(500)\n",
        "print(f'\\n TE for the given lag is {result}, with the p-value of {p}')\n",
        "# print(f'TE for the given lag is {result}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sorJTWllinsH",
        "outputId": "7e626307-9162-4eb5-cb37-768d983e12e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running through shuffle iterations: 100%|██████████| 500/500 [00:15<00:00, 32.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " TE for the given lag is 0.07802499562194608, with the p-value of 0.044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5uONVdf291XJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}